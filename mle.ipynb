{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimation (MLE).\n",
    "\n",
    "This is an overview of how to derive the likelihood from a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood function\n",
    "\n",
    "Suppose we have a set of data scattered in higher dimensions that we want to fit a multivariate normal distribution (Gaussian).\n",
    "\n",
    "The arbitrary pdf of such data is:\n",
    "\n",
    "$$p(x | \\mu, \\Sigma) = \\frac{1}{( 2 \\pi )^{D/2} \\| \\Sigma \\|^{1/2}} \\exp{\\left ( -\\frac{1}{2} (x-\\mu)^{T} \\Sigma^{-1} (x-\\mu) \\right )}$$\n",
    "\n",
    "Where $x$ and $\\mu$ are vectors and $\\Sigma$ is a matrix better represented as: $\\underline{x}, \\underline{\\mu}, \\underline{\\underline{\\Sigma}}$. And $D$ is the dimensionality of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset $X$ such that:\n",
    "\n",
    "$$X = \\left \\{ x_1, x_2, x_3, \\dots, x_N \\right \\}$$\n",
    "\n",
    "This way we can rewrite the independent probabilities as:\n",
    "\n",
    "$$p(X | \\mu, \\Sigma) = \\prod_{i=1}^{N} p(x_{i} | \\mu, \\Sigma) = \\prod_{i=1}^{N} \\frac{1}{( 2 \\pi )^{D/2} \\| \\Sigma \\|^{1/2}} \\exp{\\left ( -\\frac{1}{2} (x_{i}-\\mu)^{T} \\Sigma^{-1} (x_{i}-\\mu) \\right )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood\n",
    "\n",
    "Due to the fact that we want to maximize/minimize, it is helpful to look at the log of the probabilities which simplifies the calculations.\n",
    "\n",
    "$$L \\left ( X | \\mu, \\Sigma \\right ) = \\log \\left ( p(X | \\mu, \\Sigma) \\right ) = -\\frac{ND}{2} \\log {2 \\pi} - \\frac{N}{2} \\log{\\| \\Sigma \\|} -\\frac{1}{2} \\sum_{i=1}^{N} (x_{i}-\\mu)^{T} \\Sigma^{-1} (x_{i}-\\mu) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean estimation\n",
    "\n",
    "To minimize the log likelihood function, we will set the derivative to zero. The partial derivative will look like this:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{\\mu}} = \\sum_{i=1}^{N} (x_i - \\mu)^{T} \\Sigma^{-1} = \\left ( \\nabla_{\\mu} L \\right )^{T}$$\n",
    "\n",
    "Assuming that $\\Sigma$ is invertible, setting this partial to zero gives us:\n",
    "\n",
    "$$ \\overline{x} = \\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math refresher\n",
    "\n",
    "For this part we have to understand some properties of vector calculus that will be relevant.\n",
    "\n",
    "#### Derivative of inverted matrix.\n",
    "\n",
    "$$\\partial{\\Sigma^{-1}}$$\n",
    "\n",
    "To compute this, we will refer back to the identity of invertible matrices.\n",
    "\n",
    "$$\\Sigma \\Sigma^{-1} = I$$\n",
    "\n",
    "Taking the derivative here gives:\n",
    "\n",
    "$$\\partial{\\Sigma} \\Sigma^{-1} + \\Sigma \\partial{\\Sigma^{-1}} = 0 \\rightarrow \\partial{\\Sigma^{-1}} = - \\Sigma^{-1} \\partial{\\Sigma} \\Sigma^{-1} $$\n",
    "\n",
    "#### Derivative of determinants\n",
    "\n",
    "With a proof by induction, we can show this for an arbitrary 2x2 matrix:\n",
    "\n",
    "$$ \\left |\n",
    "\\begin{matrix}\n",
    "a(x) & b(x) \\\\\n",
    "c(x) & d(x)\n",
    "\\end{matrix}\n",
    "\\right | = a(x) d(x) - b(x) c(x)\n",
    "$$\n",
    "\n",
    "$$ \\frac{d}{dx} \\left |\n",
    "\\begin{matrix}\n",
    "a(x) & b(x) \\\\\n",
    "c(x) & d(x)\n",
    "\\end{matrix}\n",
    "\\right | = a'(x) d(x) + a(x) d'(x) - b'(x) c(x) - b(x) c'(x)\n",
    "$$\n",
    "\n",
    "If we rearrange the terms by pairing every other term, we get:\n",
    "\n",
    "$$\\left [ a'(x) d(x) - b'(x) c(x) \\right ] + \\left [a(x) d'(x) - - b(x) c'(x) \\right ] = \n",
    "\\left | \\begin{matrix}\n",
    "a'(x) & b'(x) \\\\\n",
    "c(x) & d(x)\n",
    "\\end{matrix} \\right | + \n",
    "\\left | \\begin{matrix}\n",
    "a(x) & b(x) \\\\\n",
    "c'(x) & d'(x)\n",
    "\\end{matrix} \\right |\n",
    "$$\n",
    "\n",
    "This is known as Jacobi's formula and the general formula goes like this:\n",
    "\n",
    "$$\\frac{d}{dt} \\det{A(t)} = \\det{A(t)} \\cdot \\textup{tr} \\left (  A(t)^{-1} \\cdot \\frac{dA(t)}{dt} \\right )$$\n",
    "\n",
    "Where $\\textup{tr}$ is the trace of the matrix.\n",
    "\n",
    "The trace has a nice circular property such that $\\textup{tr} (ABC) = \\textup{tr}(CBA)$\n",
    "\n",
    "_NOTE_ the trace of a scalar is the same as itself. We can see that in the generalized quadratic form using matrix algebra.\n",
    "\n",
    "$$f(x) = x^TAx = tr(x^TAx)$$\n",
    "\n",
    "### Derivative of multi-variate function with Taylor series\n",
    "\n",
    "Suppose we have the vector valued function $f(\\underline{x})$ that we wish to take the derivative of:\n",
    "\n",
    "$$f(x + \\epsilon) = f(x) + \\sum_{i} \\sum_{j} \\left ( \\left ( \\frac{\\partial f}{\\partial x} \\right )_{ij} \\epsilon_{ij} \\right ) + O(\\epsilon^2)$$\n",
    "\n",
    "It turns out that in order to convert the output to the dimensionality required for the derivative, the combined partial sums of the derivative are equal to the trace:\n",
    "\n",
    "$$\\sum_{i} \\sum_{j} \\left ( \\left ( \\frac{\\partial f}{\\partial x} \\right )_{ij} \\epsilon_{ij} \\right ) = \\textup{tr} \\left (  \\left ( \\frac{\\partial f}{\\partial x} \\right )^{T} \\epsilon \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance estimation\n",
    "\n",
    "Now that we have all the tools we need, we shall try to set the partial of the log likelihood with respect to the covariance to zero.\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\Sigma} = - \\frac{N}{2} \\Sigma^{-1} - \\frac{1}{2} \\frac{\\partial}{\\partial \\Sigma} \\sum_{i=1}^{N} (x_{i}-\\mu)^{T} \\Sigma^{-1} (x_{i}-\\mu) $$\n",
    "\n",
    "By the derivative of multi-variate functions we know that the combined partial sums evaluate to the trace:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\Sigma} = - \\frac{N}{2} \\Sigma^{-1} - \\frac{1}{2}  \\sum_{i=1}^{N}  \\textup{tr} \\left ( (-1) (x_{i}-\\mu)^{T} \\Sigma^{-1} \\Sigma^{-1} (x_{i}-\\mu)  \\right )$$\n",
    "\n",
    "We remember from the derivative of the inverse of a matrix and the circular trace result, we arrive at the following:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\Sigma} = - \\frac{N}{2} \\Sigma^{-1} - \\frac{1}{2} \\sum_{i=1}^{N} (-1) \\Sigma^{-1} (x_{i}-\\mu)  (x_{i}-\\mu)^{T} \\Sigma^{-1}$$\n",
    "\n",
    "Now we can set the derivative equal to zero:\n",
    "\n",
    "$$N\\Sigma^{-1} = \\sum_{i=1}^{N} \\Sigma^{-1} (x_{i}-\\mu) (x_{i}-\\mu)^{T} \\Sigma^{-1}$$\n",
    "\n",
    "Right and left multiply by $\\Sigma$ to get:\n",
    "\n",
    "$$ \\Sigma = \\hat{\\Sigma} = \\frac{1}{N} \\sum_{i=1}^{N} (x_{i}-\\mu) (x_{i}-\\mu)^{T}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Here is the python code that computes the MLE for a given dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Mean Vector:\n",
      "[2.8, 3.7, 4.7]\n",
      "MLE Covariance Matrix:\n",
      "[[0.325, -0.07499999999999998, -0.07499999999999998],\n",
      " [-0.07499999999999998, 0.32499999999999996, 0.32499999999999996],\n",
      " [-0.07499999999999998, 0.32499999999999996, 0.32499999999999996]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def mean_vector(data: list[list[float]]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute the mean vector of the data.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of list of floats): A list of lists where each inner list is a data point.\n",
    "\n",
    "    Returns:\n",
    "    list of floats: The mean vector.\n",
    "    \"\"\"\n",
    "    # Number of samples and features\n",
    "    num_samples = len(data)\n",
    "    num_features = len(data[0])\n",
    "\n",
    "    # Initialize mean vector with zeros\n",
    "    mean = [0] * num_features\n",
    "\n",
    "    # Calculate the sum for each feature\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_features):\n",
    "            mean[j] += data[i][j]\n",
    "\n",
    "    # Compute the mean by dividing by the number of samples\n",
    "    mean = [x / num_samples for x in mean]\n",
    "    return mean\n",
    "\n",
    "def covariance_matrix(data: list[list[float]], mean: list[float]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix of the data given the mean vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list[list[float]]\n",
    "        A list of lists where each inner list is a data point.\n",
    "    mean: list[float]\n",
    "        The mean vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[float]]\n",
    "        The covariance matrix.\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    num_features = len(data[0])\n",
    "\n",
    "    # Initialize covariance matrix with zeros\n",
    "    covariance = [[0] * num_features for _ in range(num_features)]\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            cov_sum = 0\n",
    "            for k in range(num_samples):\n",
    "                cov_sum += (data[k][i] - mean[i]) * (data[k][j] - mean[j])\n",
    "            covariance[i][j] = cov_sum / (num_samples - 1)  # Use n-1 for sample covariance\n",
    "\n",
    "    return covariance\n",
    "\n",
    "# Example usage\n",
    "data = [\n",
    "    [2.5, 3.5, 4.5],\n",
    "    [3.0, 3.0, 4.0],\n",
    "    [2.0, 4.0, 5.0],\n",
    "    [3.5, 3.5, 4.5],\n",
    "    [3.0, 4.5, 5.5]\n",
    "]\n",
    "\n",
    "mean = mean_vector(data)\n",
    "covariance = covariance_matrix(data, mean)\n",
    "\n",
    "print(\"MLE Mean Vector:\")\n",
    "pprint(mean)\n",
    "print(\"MLE Covariance Matrix:\")\n",
    "pprint(covariance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
